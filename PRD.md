# Project Overview & Goal 
The project is to ensure a fully functional and successful execution of a robotic arm picking and placing objects as per the user command. There is a depth camera placed in the workspace to ensure proper feeding of data to the arm to understand the spatial coordinates of the workspace and the objects. The user will give the command to the LLM through text, the LLM will then work with depth camera sensors output and the robotic arm and perform the task as requested by the user.

We will have to use our local Macbook which we are currently using to run the simulator and verify the results.

# Hardware Components & Services
We need to adhere to the following hardware components that are used in the actual lab.

- Robotic arm
    - Kinova Mico 2 M1N6S300
- Depth camera
    - Intel real sense depth camera D435
- LLM
    - Use Gemini API with API Key and use the Gemini 3 Flash preview model (gemini-3-flash-preview) using google-genai package
- The robotic arm works with ROS codes generated by the LLM based on the depth camera sensor data and user request provided

# Simulation
- The entire workspace should be simulated using Gazebo
- We need to exactly replicate the workspace with a table where one end of the table has the depth camera clamped and the other end has the robotic arm and on the table in the linear space between these, is the objects kept

